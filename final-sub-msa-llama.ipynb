{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":93282,"databundleVersionId":11098970,"sourceType":"competition"},{"sourceId":104449,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68809,"modelId":91102}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n!pip install -q bitsandbytes\n!pip install -q accelerate\n!pip install -q peft\n!pip install -q --upgrade transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, PeftModel, get_peft_model\nfrom datasets import load_dataset, Dataset\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_path = \"/kaggle/input/multi-lingual-sentiment-analysis/train.csv\"\n\ndf = pd.read_csv(file_path)\ndf = pd.DataFrame(df)\n\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df, val_df = train_test_split(df, test_size = 0.2, random_state = 42)\n\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\n\nprint(f\"Training dataset: {train_dataset.shape}\")\nprint(f\"Validation datset: {val_dataset.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_sentiment(df, val_df):\n\n    train_counts = df['label'].value_counts()\n    val_counts = val_df['label'].value_counts()\n    fig, ax = plt.subplots(figsize=(10,6))\n    x = range(len(train_counts))\n    width = 0.25\n    ax.bar([i- width for i in x], train_counts.values, width, label = 'Train', alpha = 0.8)\n    ax.bar([i+ width for i in x], val_counts.values, width, label='Validation', alpha = 0.8)\n    ax.set_ylabel('Count')\n    ax.set_title('Sentiment distribution')\n    ax.set_xticks(x)\n    ax.set_xticklabels(train_counts.index)\n    ax.legend()\n\n    for i, v in enumerate(train_counts.values):\n        ax.text(i - width, v, str(v), ha='center', va='bottom')\n    for i, v in enumerate(val_counts.values):\n        ax.text(i + width, v, str(v), ha='center', va='bottom')\n\n    plt.tight_layout()\n    plt.show()\n\n# Call the function to plot\nplot_sentiment(df, val_df)\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import f1_score, classification_report, confusion_matrix\n\ndef evaluate_binary_sentiment(y_true, y_pred):\n    \"\"\"\n    Evaluate binary sentiment classification performance using F1 score.\n    \n    Parameters:\n    y_true (array-like): Ground truth labels ('positive' or 'negative')\n    y_pred (array-like): Predicted labels ('positive' or 'negative')\n    \n    Returns:\n    None (prints evaluation metrics)\n    \"\"\"\n    # Define mapping for binary sentiment\n    mapping = {'positive': 1, 'negative': 0}\n    \n    # Convert string labels to numeric\n    def map_func(x):\n        return mapping.get(x.lower())  # default to negative if unknown label\n    \n    y_true = np.vectorize(map_func)(y_true)\n    y_pred = np.vectorize(map_func)(y_pred)\n    \n    # Calculate overall F1 score\n    f1 = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n    print(f'Overall F1 Score: {f1:.3f}')\n    \n    # Calculate F1 scores for each class\n    f1_pos = f1_score(y_true=y_true, y_pred=y_pred, pos_label=1)\n    f1_neg = f1_score(y_true=y_true, y_pred=y_pred, pos_label=0)\n    print(f'F1 Score for positive sentiment: {f1_pos:.3f}')\n    print(f'F1 Score for negative sentiment: {f1_neg:.3f}')\n    \n    # Generate classification report\n    print('\\nClassification Report:')\n    print(classification_report(y_true=y_true, y_pred=y_pred, \n                              target_names=['negative', 'positive']))\n    \n    # Generate confusion matrix\n    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred)\n    print('\\nConfusion Matrix:')\n    print('                 Predicted Negative  Predicted Positive')\n    print(f'Actual Negative      {conf_matrix[0][0]:<18d}{conf_matrix[0][1]}')\n    print(f'Actual Positive      {conf_matrix[1][0]:<18d}{conf_matrix[1][1]}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nmodel_path = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\n\n\n# Quantization configuration\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\n# Loading the model and tokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path,quantization_config=bnb_config,\n                                             device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\n    model_path,\n    model_max_length=1024,\n    padding_side=\"left\",\n    add_eos_token=True)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"map_dict = {\n    \"as\": \"Assamese\",\n    \"bd\": \"Bodo\",\n    \"bn\": \"Bengali\",\n    \"gu\": \"Gujarati\",\n    \"hi\": \"Hindi\",\n    \"kn\": \"Kannada\",\n    \"ml\": \"Malayalam\",\n    \"mr\": \"Marathi\",\n    \"or\": \"Odia\",\n    \"pa\": \"Punjabi\",\n    \"ta\": \"Tamil\",\n    \"te\": \"Telugu\",\n    \"ur\": \"Urdu\"\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ndef generate_response(prompt,  max_length=200):\n    # Tokenize input prompt and move to the correct device\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n    # Generate text\n    with torch.no_grad():\n        output = model.generate(**inputs)\n\n    # Decode and return generated text\n    return tokenizer.decode(output[0], skip_special_tokens=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lang = 'bn'\nbeng_sen = list(df[df[\"language\"] == lang]['sentence'])\nprint(beng_sen[8])\n\nprompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nYou are a helpful assistant that analyzes sentiment in text.<|eot_id|><|start_header_id|>user<|end_header_id|>\nAnalyze the sentiment of the following {lang} text and respond with exactly one word (either 'positive' or 'negative'):\n{beng_sen[8]}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n\nprint(generate_response(prompt))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\nfrom tqdm import tqdm\nimport re\ndef predict_sentiment(test_dataset, model, tokenizer):\n    \"\"\"\n    Predicts sentiments for multilingual text data.\n    \n    Args:\n        test_dataset: Dataset containing the test examples\n        model: Fine-tuned model\n        tokenizer: Associated tokenizer\n    \n    Returns:\n        list: Predicted sentiments ('positive' or 'negative')\n    \"\"\"\n    y_pred = []\n    \n    # Create pipeline once outside the loop for efficiency\n    \"\"\"pipe = pipeline(\n        task=\"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        max_new_tokens=5,\n        temperature=0.1,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    \"\"\"\n\n    \n\n    \n\n    \n    for index,example in test_dataset.iterrows():\n        # Format prompt similar to training data\n        #language_name = map_dict.get(example['language'], example['language'])\n        #prompt = f\"Analyze the sentiment of the following {language_name} text:\\nText: {example['sentence']}\\nSentiment:\"\n        \n        \"\"\"\n        prompt = f<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n        You are a helpful assistant that analyzes sentiment in text.<|eot_id|><|start_header_id|>user<|end_header_id|>\n        Analyze the sentiment of the following {language_name} text and respond with exactly one word (either 'positive' or 'negative'):\n        {example['sentence']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n        \"\"\"\n        #print(example['sentence'])\n        prompt = f\"\"\"\n        <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n        \n        You are a helpful AI assistant for sentiment analysis(POSITIVE or NEGATIVE)<|eot_id|>\n        <|start_header_id|>user<|end_header_id|>\n        \n        Predict the sentiment of this sentence: {example['sentence']}\n        Output Format: POSITIVE or NEGATIVE\n        <|eot_id|>\n        <|start_header_id|>assistant<|end_header_id|>\n        \"\"\"\n        \n        # Generate prediction\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n        #result = pipe(prompt)[0]['generated_text']\n        # Generate text\n        with torch.no_grad():\n            output = model.generate(**inputs)\n        result= tokenizer.decode(output[0], skip_special_tokens=True)\n        # Extract the predicted sentiment from the generated text\n        # Look for the sentiment after the last occurrence of \"Sentiment:\"\n        \n        sentiment_part = result.split(\"\\n\")[-1].strip()\n        print(sentiment_part)\n        \n        \"\"\"\n        \n        if \"positive\" in sentiment_part.lower():\n            y_pred.append(\"positive\")\n        elif \"negative\" in sentiment_part.lower():\n            y_pred.append(\"negative\")\n        else:\n            y_pred.append(\"negative\")  # Default to negative for unrecognized outputs\n        \"\"\"\n        #match = re.search(r'\\b(POSITIVE|NEGATIVE)\\b', sentiment_part[0], re.IGNORECASE)\n        #out =  match.group(0).upper() if match else \"POSITIVE\"\n        y_pred.append(sentiment_part)\n    \n    y_pred_final = []\n    count = 0\n    case_issue=0\n    for i in y_pred:\n        if i != \"POSITIVE\" and i!= \"NEGATIVE\":\n            if i == 'positive' or i == 'negative':\n                y_pred_final.append(i.upper())\n                case_issue+=1\n            else:\n                y_pred_final.append(\"POSITIVE\")\n                count += 1\n        else:\n            y_pred_final.append(i)\n    print(f\"Total rouge output: {count}\")\n    print(f\"Total Case Issue: {case_issue}\")\n\n        \n        \n    \n    return y_pred_final","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_dataset(dataset):\n    \n    def format_prompt(example):\n        language_name = map_dict.get(example['language'], example['language'])\n        \"\"\"\n        formatted_text = (\n            f\"Analyze the sentiment of the following {language_name} text:\\n\"\n            f\"Text: {example['sentence']}\\n\"\n            f\"Sentiment: {example['label']}\"\n        )\n        \"\"\"\n\n\n        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n        You are a helpful assistant that analyzes sentiment in text.<|eot_id|><|start_header_id|>user<|end_header_id|>\n        Analyze the sentiment of the following {language_name} text and respond with exactly one word (either 'positive' or 'negative'):\n        {example['sentence']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n        <|response_tag|> {example['label']} <|eom_id|>\"\"\"\n        #return {\"text\": formatted_text}\n        return tokenizer(\n            prompt, \n            padding=\"max_length\",  # Ensures uniform length\n            truncation=True,       # Avoids exceeding max length\n            max_length=1024,       # Matches model max length\n            return_tensors=\"pt\"\n        )\n    \n    # Map the formatting function across the dataset\n    formatted_dataset = dataset.map(format_prompt)#, batched = True)\n    return formatted_dataset\n\n# Format datasets\ntrain_data = prepare_dataset(train_dataset)\neval_data = prepare_dataset(val_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install trl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import LoraConfig\nfrom transformers import TrainingArguments\nfrom trl import SFTTrainer, SFTConfig\nimport os \n# Define output directory\noutput_dir = \"trained_weights\"\nos.makedirs(output_dir, exist_ok=True)\n# Memory-efficient LoRA Configuration\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=16,  # Reduced from 64 to save memory\n    bias=\"none\",\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    task_type=\"CAUSAL_LM\",\n)\n\n# Memory-efficient Training Arguments\ntraining_arguments = SFTConfig(\n    output_dir=output_dir,\n    num_train_epochs=10,\n    per_device_train_batch_size=1,  # Keep at 1 for memory efficiency\n    gradient_accumulation_steps=4,   # Reduced from 8 to speed up training\n    gradient_checkpointing=True,     # Keep this for memory efficiency\n    optim=\"paged_adamw_32bit\",\n    save_steps=0,\n    # Increased to reduce logging overhead\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\",\n    report_to=None,\n    evaluation_strategy=\"no\",     # Disabled evaluation to save memory\n    max_seq_length = 1024,\n    packing = True,\n    dataset_text_field=\"sentence\",\n    logging_strategy=\"epoch\",\n    #padding = True,\n    #truncation = True,\n    #output_dir = output_dir,\n    dataset_kwargs = {\n        \"add_special_tokens\" : False,\n        \"append_concat_token\" : False,\n    }\n)\n\n# Initialize SFT Trainer\ntrainer = SFTTrainer(\n    model=model,\n    args=training_arguments,\n    #training_args=SFTConfig(max_seq_length = 1024)\n    train_dataset=train_data,\n    eval_dataset = eval_data,\n    peft_config=peft_config,\n    #dataset_text_field=\"prompt\",\n    tokenizer=tokenizer,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\nwandb.login(key='3ecf0adab295dff557ad09ec279a375e3131ad74')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\nwandb.init(project=\"llamasftmsa\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.save_model()  # Save model and tokenizer\ntokenizer.save_pretrained(output_dir)  # Save tokenizer (optional)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print the overall training state:\nprint(\"Global step:\", trainer.state.global_step)\nprint(\"Epoch:\", trainer.state.epoch)\n\n# See all logged metrics (each log is a dict with keys such as loss, learning_rate, etc.)\nprint(\"Log history:\", trainer.state.log_history)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\n\ndel [\n    model, \n     tokenizer, \n     peft_config, \n    trainer, \n    train_data, \n    eval_data, \n    bnb_config, \n    \n    training_arguments\n]\n# del [df, X_train, X_eval]\ndel [\n    TrainingArguments, \n    SFTTrainer, \n    LoraConfig, \n    BitsAndBytesConfig\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for _ in range(100):\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import (AutoConfig,\n                          AutoModelForCausalLM, \n                          AutoTokenizer,\n                          BitsAndBytesConfig, \n                          TrainingArguments, \n                          pipeline) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Specify model name and path\nmodel_name = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\npeft_model_id = \"/kaggle/working/trained_weights\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    llm_int8_enable_fp32_cpu_offload=True\n)\n\n# Loading the model and tokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path,quantization_config=bnb_config,\n                                             device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\n    model_path,\n    model_max_length=1024,\n    padding_side=\"left\",\n    add_eos_token=True)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(tokenizer))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the PEFT adapter\nmodel.load_adapter(peft_model_id)\n\n# Configure model settings\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Ensure the model is in evaluation mode\nmodel.eval()\n\nprint(\"Model and tokenizer loaded successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ndef generate_response(prompt,  max_length=200):\n    # Tokenize input prompt and move to the correct device\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n    # Generate text\n    with torch.no_grad():\n        output = model.generate(**inputs)\n\n    # Decode and return generated text\n    return tokenizer.decode(output[0], skip_special_tokens=True)\nlang = 'bn'\nbeng_sen = list(val_df[val_df[\"language\"] == lang]['sentence'])\n\nprint(beng_sen[5])\nprompt = f\"\"\"\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful AI assistant for sentiment analysis(POSITIVE or NEGATIVE)<|eot_id|>\n<|start_header_id|>user<|end_header_id|>\n\nPredict the sentiment of this sentence: {beng_sen[5]}\nOutput Format: POSITIVE or NEGATIVE\n<|eot_id|>\n<|start_header_id|>assistant<|end_header_id|>\n\"\"\"\n\nprint(generate_response(prompt))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#sample_val = val_df.sample(n = 20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#y_pred = predict_sentiment(sample_val, model,tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n#y_true_val = list(sample_val['label'].copy())\n\n#evaluate_binary_sentiment(y_true_val, y_pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_file_path = \"/kaggle/input/multi-lingual-sentiment-analysis/test.csv\"\n\ntest_df = pd.read_csv(test_file_path)\ntest_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataframe = pd.DataFrame(test_df)\ntest_dataset = Dataset.from_pandas(test_dataframe)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = predict_sentiment(test_dataframe, model, tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions_df = pd.DataFrame({\n    'ID': test_df['ID'],\n    #'sentence': test_df['sentence'],\n    'label': predictions\n})\n\n# Capitalize the first letter of each label\npredictions_df['label'] = predictions_df['label'].str.capitalize()\n\npredictions_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save predictions if needed\npredictions_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}